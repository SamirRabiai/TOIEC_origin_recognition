{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samir RABIAI M2 ISD\n",
    "\n",
    "Les cellules pour tester notre modèle le plus performant avec les données test sont à la fin du NoteBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliothèques utilisées\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification du fichier pour avoir le format voulu\n",
    "\n",
    "with open('train.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    with open('train.txt', \"w\") as file:\n",
    "        for line in lines:\n",
    "            file.write(re.sub(r\"\\(([A-Z]+)\\) (.+)\", r'\\1;\"\\2\"', line))\n",
    "# Upload du fichier\n",
    "df = pd.read_table(\"train.txt\", sep=\";\", names=[\"origin\",\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GER</td>\n",
       "      <td>IThe importance and popularity of travelling i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TUR</td>\n",
       "      <td>It is an important decision , how to plan your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHI</td>\n",
       "      <td>Some people believe that young people can enjo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEL</td>\n",
       "      <td>Travelling is usually considered as good recre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARA</td>\n",
       "      <td>i agree that . Life is a person live period of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  origin                                               text\n",
       "0    GER  IThe importance and popularity of travelling i...\n",
       "1    TUR  It is an important decision , how to plan your...\n",
       "2    CHI  Some people believe that young people can enjo...\n",
       "3    TEL  Travelling is usually considered as good recre...\n",
       "4    ARA  i agree that . Life is a person live period of..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9900</td>\n",
       "      <td>9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>11</td>\n",
       "      <td>9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>GER</td>\n",
       "      <td>IThe importance and popularity of travelling i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       origin                                               text\n",
       "count    9900                                               9900\n",
       "unique     11                                               9900\n",
       "top       GER  IThe importance and popularity of travelling i...\n",
       "freq      900                                                  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GER', 'TUR', 'CHI', 'TEL', 'ARA', 'SPA', 'HIN', 'JPN', 'KOR',\n",
       "       'FRE', 'ITA'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"origin\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GER    900\n",
       "TUR    900\n",
       "CHI    900\n",
       "TEL    900\n",
       "ARA    900\n",
       "SPA    900\n",
       "HIN    900\n",
       "JPN    900\n",
       "KOR    900\n",
       "FRE    900\n",
       "ITA    900\n",
       "Name: origin, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"origin\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\samra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Bibliothèques à utiliser\n",
    "import contractions\n",
    "\n",
    "import gensim\n",
    "\n",
    "#pour lemmatiser nos mots\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préprocessing classique\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #lowercase\n",
    "    res_text = text.lower()\n",
    "    #contractions\n",
    "    res_text = contractions.fix(res_text)\n",
    "    #remove punctuation\n",
    "    res_text = re.sub(r'[^\\w\\s]', '', res_text)\n",
    "    #remove stop words\n",
    "    res_text = gensim.parsing.remove_stopwords(res_text)\n",
    "    #lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    res_text = nltk.word_tokenize(res_text)\n",
    "    res_text = ' '.join([lemmatizer.lemmatize(w) for w in res_text])\n",
    "    #remove words that are 2 letters or less\n",
    "    res_text = res_text.split(' ')\n",
    "    res_text = [word for word in res_text if len(word) > 2]\n",
    "    res_text = \" \".join(res_text)\n",
    "    return res_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = [clean_text(df.iloc[i]['text']) for i in range(len(df))]\n",
    "df['cleaned_text'] = cleaned_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèles simples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Régression logistique avec CountVectorizer et TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding \n",
    "# Utilisation des algo CountVecterozier et TF-IDF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "X = df['cleaned_text'].values.tolist()\n",
    "Y = df['origin'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TF IDF\n",
    "tf = TfidfVectorizer(max_df= 0.8)\n",
    "tf_X = tf.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=2000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Utilisation du modèle de régression logistique\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tf_train = tf.transform(X_train)\n",
    "tf_test = tf.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression( max_iter= 2000)\n",
    "tfidf_lr = lr.fit(tf_train, y_train)\n",
    "\n",
    "\n",
    "print(tfidf_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_y_pred = tfidf_lr.predict(tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with TF IDF Embedding\n",
      "accuracy :  0.6348484848484849\n",
      "f1_score :  0.6338006026358741\n",
      "precision score :  0.6344140477090864\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Logistic Regression with TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, tf_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, tf_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, tf_y_pred, average= 'macro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.8)\n",
    "cv_X = cv.fit_transform(X)\n",
    "cv_train = cv.transform(X_train)\n",
    "cv_test = cv.transform(X_test)\n",
    "cv_lr = lr.fit(cv_train, y_train)\n",
    "cv_y_pred = cv_lr.predict(cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with CountVectorizer Embedding\n",
      "accuracy :  0.6338383838383839\n",
      "f1_score :  0.6333020588788121\n",
      "precision score :  0.6341029046192708\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression with CountVectorizer Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, cv_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, cv_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, cv_y_pred, average= 'macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM avec TF-IDF et CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle SVM avec TF IDF\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 29)\n",
    "classifier.fit(tf_train, y_train)\n",
    "#Prediction sur le Test set\n",
    "tf_y_pred = classifier.predict(tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with TF IDF Embedding\n",
      "accuracy :  0.6454545454545455\n",
      "f1_score :  0.6454749463817382\n",
      "precision score :  0.6485534180766808\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"SVM with TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, tf_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, tf_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, tf_y_pred, average= 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(cv_train, y_train)\n",
    "#Prediction sur le Test set\n",
    "cv_y_pred = classifier.predict(cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with CountVectorizer Embedding\n",
      "accuracy :  0.5595959595959596\n",
      "f1_score :  0.5580680258051905\n",
      "precision score :  0.5608032337650032\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"SVM with CountVectorizer Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, cv_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, cv_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, cv_y_pred, average= 'macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression logistique avec Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\samra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de modèle pré-entrainé\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction de notre corpus\n",
    "corpus = [sent_tokenize(text) for text in df.cleaned_text.values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ithe importance popularity travelling raising everybody interested getting know country continent way differently agree statement best way travel group led tour guide travel group tour guide know famous place know interesting thing country decided visit tour guide organise thing appointment group aspect know lot interesting people travel group easy know people usually sit bus sleep hotel like borring travel group decide travel group visit country dangerouses tour guide inform dangerouses avoid risk special rule thing bring trouble follow furthermore tour guide help communication problem able speak language people country visit speak tour guide help able speak visit doctor police happended ask tour guide able help reason prefer tavel group tour guide']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons remplir deux listes, une avec les mots constituants le corpus, l'autre avec les labels\n",
    "# words va nous servir à initialiser notre modèle avec une fenetre de 3 et une répetition minimum de mots de 1\n",
    "\n",
    "words = []\n",
    "labels = []\n",
    "for txt,label in zip(df.cleaned_text.values.tolist(), df.origin.values.tolist()):\n",
    "    sent_txt = sent_tokenize(txt)\n",
    "    for word in sent_txt:\n",
    "        words.append(simple_preprocess(word))\n",
    "        labels.append(label)\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(words, window = 3, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9900\n",
      "9900\n"
     ]
    }
   ],
   "source": [
    "model_w2v.corpus_count\n",
    "print(len(words))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embeddings(text,model_w2v, method:str = 'avg'):\n",
    "    \"\"\" \n",
    "    Return the Text vector using the average or sum of word embeddings given by Word2Vec\n",
    "    \"\"\"\n",
    "    if method == 'avg':\n",
    "        return np.mean([model_w2v[word] for word in text if word in model_w2v.index_to_key], axis = 0)\n",
    "    return np.sum([model_w2v[word] for word in text if word in model_w2v.index_to_key], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GER', 'TUR', 'CHI', 'TEL', 'ARA', 'SPA', 'HIN', 'JPN', 'KOR',\n",
       "       'FRE', 'ITA'], dtype=object)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"origin\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons créer nos vecteurs en utilisant la moyenne des word embeddings donnés par le modèle \n",
    "# A noter que cette cellule prend environs 25 min à s'executer \n",
    "X_texts = [text_embeddings(text,wv) for text in words]\n",
    "X_texts = np.array(X_texts)\n",
    "label_dict  ={'GER':0, 'TUR':1,'CHI': 2, 'TEL':3,'ARA': 4, 'SPA':5, 'HIN':6,'JPN': 7, 'KOR':8,'FRE': 9, 'ITA': 10}\n",
    "y_texts = [label_dict[label] for label in labels]\n",
    "y_texts = y_texts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_texts, y_texts, test_size =0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900, 300)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons voir la performance du modèle de regression logistique avec les vecteurs Word2Vec (embedding vectors)\n",
    "lr_w2v = LogisticRegression(max_iter=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_corr = []\n",
    "y_test_corr = []\n",
    "for i,y in zip(X_test,y_test):\n",
    "    X_test_corr.append(i)\n",
    "    y_test_corr.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_corr = []\n",
    "y_train_corr = []\n",
    "for i,y in zip(X_train,y_train):\n",
    "    X_train_corr.append(i)\n",
    "    y_train_corr.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_corr = np.array(X_train_corr)\n",
    "X_test_corr = np.array(X_test_corr)\n",
    "\n",
    "np.shape(y_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=20000)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_corr = np.array(y_train_corr)\n",
    "lr_w2v.fit(X_train_corr, y_train_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  4, 10, ...,  1,  8,  9])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_w2v.predict(X_test_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3712121212121212"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_corr,lr_w2v.predict(X_test_corr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy word2vec avec window = 10 et logisticRegression  0.38282828282828285 d'accuracy et 66 min d'éxecution \n",
    "\n",
    "avec window = 3   62 min accuracy score = 0.3712121212121212\n",
    "                  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications preprocessing (pas de stemming ni lemmatization, laisser les stopwords) \n",
    "Nous allons refaire certains étapes précedentes mais cette fois ci avec un preprocessing plus adapté à notre tâche \\\n",
    "Vous pouvez commencer à exectuer à partir de cette cellule ou relancer le kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliothèques utilisées\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import re\n",
    "import csv\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification du fichier pour avoir le format voulu\n",
    "\n",
    "with open('train.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    with open('train.txt', \"w\") as file:\n",
    "        for line in lines:\n",
    "            file.write(re.sub(r\"\\(([A-Z]+)\\) (.+)\", r'\\1;\"\\2\"', line))\n",
    "# Upload du fichier\n",
    "df = pd.read_table(\"train.txt\", sep=\";\", names=[\"origin\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "# Bibliothèques à utiliser\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #lowercase\n",
    "    res_text = text.lower()\n",
    "    #contactions\n",
    "    res_text = contractions.fix(res_text)\n",
    "    #remove punctuation\n",
    "    res_text = re.sub(r'[^\\w\\s]', '', res_text)\n",
    "\n",
    "    return res_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = [clean_text(df.iloc[i]['text']) for i in range(len(df))]\n",
    "df['cleaned_text'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = df['cleaned_text'].values.tolist()\n",
    "Y = df['origin'].values.tolist()\n",
    "# TF IDF\n",
    "tf = TfidfVectorizer(max_df= 0.8)\n",
    "tf_X = tf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=2000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Utilisation du modèle de régression logistique\n",
    "\n",
    "\n",
    "tf_train = tf.transform(X_train)\n",
    "tf_test = tf.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression( max_iter= 2000)\n",
    "tfidf_lr = lr.fit(tf_train, y_train)\n",
    "\n",
    "\n",
    "print(tfidf_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with TF IDF Embedding\n",
      "accuracy :  0.6722222222222223\n",
      "f1_score :  0.6712688377376596\n",
      "precision score :  0.6727870659818613\n"
     ]
    }
   ],
   "source": [
    "tf_y_pred = tfidf_lr.predict(tf_test)\n",
    "from sklearn import metrics\n",
    "print(\"Logistic Regression with TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, tf_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, tf_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, tf_y_pred, average= 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression logistique CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "cv = CountVectorizer(max_df=0.8)\n",
    "cv_X = cv.fit_transform(X)\n",
    "cv_train = cv.transform(X_train)\n",
    "cv_test = cv.transform(X_test)\n",
    "cv_lr = lr.fit(cv_train, y_train)\n",
    "cv_y_pred = cv_lr.predict(cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with CountVectorizer Embedding\n",
      "accuracy :  0.6818181818181818\n",
      "f1_score :  0.6807873532443216\n",
      "precision score :  0.6815225284771577\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression with CountVectorizer Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, cv_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, cv_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, cv_y_pred, average= 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with TF IDF\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 29)\n",
    "classifier.fit(tf_train, y_train)\n",
    "#Prediction sur le Test set\n",
    "tf_y_pred = classifier.predict(tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with TF IDF Embedding\n",
      "accuracy :  0.6944444444444444\n",
      "f1_score :  0.694127236780096\n",
      "precision score :  0.6966409596481816\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM with TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, tf_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, tf_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, tf_y_pred, average= 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with CountVectorizer Embedding\n",
      "accuracy :  0.6232323232323232\n",
      "f1_score :  0.6222511201629963\n",
      "precision score :  0.624211558453877\n"
     ]
    }
   ],
   "source": [
    "# SVM with CountVectorizer\n",
    "classifier.fit(cv_train, y_train)\n",
    "#Prediction sur le Test set\n",
    "cv_y_pred = classifier.predict(cv_test)\n",
    "print(\"SVM with CountVectorizer Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, cv_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, cv_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, cv_y_pred, average= 'macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N gram Embedding (BI gram et Tri gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N gram Vectorisation with CountVectorizer\n",
    "# Bi Gram\n",
    "\n",
    "X = df['cleaned_text'].values.tolist()\n",
    "Y = df['origin'].values.tolist()\n",
    "# Utilisation du modèle de régression logistique\n",
    "cv = CountVectorizer(max_df=0.8, ngram_range=(2,2))\n",
    "cv_X = cv.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=29)\n",
    "cv_train = cv.transform(X_train)\n",
    "cv_test = cv.transform(X_test)\n",
    "cv_lr = lr.fit(cv_train, y_train)\n",
    "cv_y_pred = cv_lr.predict(cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with BiGram CountVectorizer Embedding\n",
      "accuracy :  0.694949494949495\n",
      "f1_score :  0.694488153208655\n",
      "precision score :  0.6945401001683469\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression with BiGram CountVectorizer Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, cv_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, cv_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, cv_y_pred, average= 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(3,3), max_df=0.8)\n",
    "cv_X = cv.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=29)\n",
    "cv_train = cv.transform(X_train)\n",
    "cv_test = cv.transform(X_test)\n",
    "cv_lr = lr.fit(cv_train, y_train)\n",
    "cv_y_pred = cv_lr.predict(cv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with TriGram CountVectorizer Embedding\n",
      "accuracy :  0.5782828282828283\n",
      "f1_score :  0.5775888713824447\n",
      "precision score :  0.5830801012078141\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression with TriGram CountVectorizer Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, cv_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, cv_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, cv_y_pred, average= 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression logistique avec BiGram TF IDF\n",
    "tf = TfidfVectorizer(max_df=0.8, ngram_range=(2,2))\n",
    "tf_X = tf.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=29)\n",
    "tf_train = tf.transform(X_train)\n",
    "tf_test = tf.transform(X_test)\n",
    "tf_lr = lr.fit(tf_train, y_train)\n",
    "tf_y_pred = tf_lr.predict(tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with BiGram TF IDF Embedding\n",
      "accuracy :  0.6707070707070707\n",
      "f1_score :  0.6712451447645873\n",
      "precision score :  0.6784874610533368\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression with BiGram TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, tf_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, tf_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, tf_y_pred, average= 'macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM with TF IDF Bi Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_text'].values.tolist()\n",
    "Y = df['origin'].values.tolist()\n",
    "tf = TfidfVectorizer(max_df=0.8, ngram_range=(2,2))\n",
    "tf_X = tf.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=29)\n",
    "tf_train = tf.transform(X_train)\n",
    "tf_test = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(tf_train, y_train)\n",
    "tf_y_pred = classifier.predict(tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with TriGram TF IDF Embedding\n",
      "accuracy :  0.591919191919192\n",
      "f1_score :  0.5926862532512902\n",
      "precision score :  0.6007423570182614\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "print(\"SVM with TriGram TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, tf_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, tf_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, tf_y_pred, average= 'macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modèles simples avec Cross validation \\\n",
    "Possible de relancer le kernel ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM TFIDF BIGRAM CROSS VALIDATION\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import re\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification du fichier pour avoir le format voulu\n",
    "\n",
    "with open('train.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    with open('train.txt', \"w\") as file:\n",
    "        for line in lines:\n",
    "            file.write(re.sub(r\"\\(([A-Z]+)\\) (.+)\", r'\\1;\"\\2\"', line))\n",
    "# Upload du fichier\n",
    "df = pd.read_table(\"train.txt\", sep=\";\", names=[\"origin\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Preprocessing\n",
    "# Bibliothèques à utiliser\n",
    "import contractions\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #lowercase\n",
    "    res_text = text.lower()\n",
    "    #contactions\n",
    "    res_text = contractions.fix(res_text)\n",
    "    #remove punctuation\n",
    "    res_text = re.sub(r'[^\\w\\s]', '', res_text)\n",
    "\n",
    "    return res_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_text = [clean_text(df.iloc[i]['text']) for i in range(len(df))]\n",
    "df['cleaned_text'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,2), max_df=0.8)\n",
    "X = df['cleaned_text'].values.tolist()\n",
    "Y = df['origin'].values.tolist()\n",
    "tfidf_X = tfidf.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation du modèle SVM\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold cross validation\n",
    "K_folds = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = cross_val_score(classifier, tfidf_X, Y, cv=K_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Scores:  [0.68989899 0.72525253 0.72424242 0.71515152 0.71717172 0.72525253\n",
      " 0.72121212 0.73737374 0.73333333 0.72121212]\n",
      "Average CV Score:  0.721010101010101\n",
      "Number of CV Scores used in Average:  10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X = df['cleaned_text'].values.tolist()\n",
    "Y = df['origin'].values.tolist()\n",
    "tf = TfidfVectorizer(max_df=0.8, ngram_range=(2,2))\n",
    "tf_X = tf.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=29)\n",
    "tf_train = tf.transform(X_train)\n",
    "tf_test = tf.transform(X_test)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(tf_train.toarray(), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_y_pred = clf.predict(tf_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes TF IDF Embedding\n",
      "accuracy :  0.40555555555555556\n",
      "f1_score :  0.40388955339368027\n",
      "precision score :  0.4239375565869525\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, nb_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, nb_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, nb_y_pred, average= 'macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_text'].values.tolist()\n",
    "Y = df['origin'].values.tolist()\n",
    "tf = TfidfVectorizer()\n",
    "tf_X = tf.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=29)\n",
    "tf_train = tf.transform(X_train)\n",
    "tf_test = tf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train = tf_train.toarray()\n",
    "y_train = np.array(y_train)\n",
    "tf_test = tf_test.toarray()\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7920"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tf_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128,input_shape=(np.shape(tf_train) ), activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(11, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tensorflow.keras.optimizers.legacy.Adam(learning_rate=1e-3, decay=1e-5)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 7920, 53814), found shape=(None, 53814)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1196/2336666247.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Cette cellule contient une erreur\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 7920, 53814), found shape=(None, 53814)\n"
     ]
    }
   ],
   "source": [
    "# Cette cellule contient une erreur \n",
    "\n",
    "model.fit(tf_train, y_train, epochs= 3, validation_data=(tf_test, y_test) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELES EN CASCADE \\\n",
    "Restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliothèques utilisées\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import re\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification du fichier pour avoir le format voulu\n",
    "\n",
    "with open('train.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    with open('train.txt', \"w\") as file:\n",
    "        for line in lines:\n",
    "            file.write(re.sub(r\"\\(([A-Z]+)\\) (.+)\", r'\\1;\"\\2\"', line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload du fichier\n",
    "df = pd.read_table(\"train.txt\", sep=\";\", names=[\"origin\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "# Bibliothèques à utiliser\n",
    "import contractions\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #lowercase\n",
    "    res_text = text.lower()\n",
    "    #contactions\n",
    "    res_text = contractions.fix(res_text)\n",
    "    #remove punctuation\n",
    "    res_text = re.sub(r'[^\\w\\s]', '', res_text)\n",
    "\n",
    "    return res_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = [clean_text(df.iloc[i]['text']) for i in range(len(df))]\n",
    "df['cleaned_text'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(2,2), max_df= 0.8)\n",
    "X = df['cleaned_text'].values.tolist()\n",
    "Y = df['origin'].values.tolist()\n",
    "tfidf_X = tfidf.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=29)\n",
    "\n",
    "\n",
    "tfidf_train = tfidf.transform(X_train)\n",
    "tfidf_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "# Utilisation du modèle SVM\n",
    "\n",
    "classifier = SVC(kernel = 'linear', random_state = 29)\n",
    "classifier.fit(tfidf_train, y_train)\n",
    "#Prediction sur le Test set\n",
    "y_pred = classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with BiGram TF IDF Embedding\n",
      "accuracy :  0.7156565656565657\n",
      "f1_score :  0.7167747347459575\n",
      "precision score :  0.7224884765980538\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"SVM with BiGram TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, y_pred, average= 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changement de labels\n",
    "\n",
    "# Réunir les langues qui font partie de la même famille\n",
    "# Faire une matrice de confusion pour voir les similarité\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(y_test, y_pred, labels=classifier.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x21be427a910>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\n",
    "\n",
    "cm_disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_family(lang):\n",
    "    if lang == 'HIN' or lang =='TEL':\n",
    "        return 'HIN/TEL'\n",
    "    if lang == 'CHI' or lang == 'JPN' or lang == 'KOR':\n",
    "        return 'CHI/JPN/KOR'\n",
    "    if lang == 'SPA' or lang == 'ITA' or lang == 'FRE' or lang == 'GER':\n",
    "        return 'FRE/GER/SPA/ITA'\n",
    "    #ARA or TUR\n",
    "    return 'ARA/TUR'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "family = [language_family(df.iloc[i]['origin']) for i in range(len(df))]\n",
    "df['family'] = family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GER</td>\n",
       "      <td>IThe importance and popularity of travelling i...</td>\n",
       "      <td>ithe importance and popularity of travelling i...</td>\n",
       "      <td>FRE/GER/SPA/ITA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TUR</td>\n",
       "      <td>It is an important decision , how to plan your...</td>\n",
       "      <td>it is an important decision  how to plan your ...</td>\n",
       "      <td>ARA/TUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHI</td>\n",
       "      <td>Some people believe that young people can enjo...</td>\n",
       "      <td>some people believe that young people can enjo...</td>\n",
       "      <td>CHI/JPN/KOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEL</td>\n",
       "      <td>Travelling is usually considered as good recre...</td>\n",
       "      <td>travelling is usually considered as good recre...</td>\n",
       "      <td>HIN/TEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARA</td>\n",
       "      <td>i agree that . Life is a person live period of...</td>\n",
       "      <td>i agree that  life is a person live period of ...</td>\n",
       "      <td>ARA/TUR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  origin                                               text  \\\n",
       "0    GER  IThe importance and popularity of travelling i...   \n",
       "1    TUR  It is an important decision , how to plan your...   \n",
       "2    CHI  Some people believe that young people can enjo...   \n",
       "3    TEL  Travelling is usually considered as good recre...   \n",
       "4    ARA  i agree that . Life is a person live period of...   \n",
       "\n",
       "                                        cleaned_text           family  \n",
       "0  ithe importance and popularity of travelling i...  FRE/GER/SPA/ITA  \n",
       "1  it is an important decision  how to plan your ...          ARA/TUR  \n",
       "2  some people believe that young people can enjo...      CHI/JPN/KOR  \n",
       "3  travelling is usually considered as good recre...          HIN/TEL  \n",
       "4  i agree that  life is a person live period of ...          ARA/TUR  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1er modele\n",
    "# Testons notre SVM avec BiGram Tf idf pour ce problème\n",
    "\n",
    "tfidf_1 = TfidfVectorizer(ngram_range=(2,2), max_df=0.8)\n",
    "X = df['cleaned_text'].values.tolist()\n",
    "y_family = df['family'].values.tolist()\n",
    "tfidf_1_X = tfidf_1.fit_transform(X)\n",
    "X_train, X_test, y_family_train, y_family_test = train_test_split(X,y_family, test_size=0.2, random_state=29)\n",
    "\n",
    "tfidf_1_train = tfidf_1.transform(X_train)\n",
    "tfidf_1_test = tfidf_1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = 'linear', random_state = 29)\n",
    "classifier.fit(tfidf_1_train, y_family_train)\n",
    "#Prediction sur le Test set\n",
    "y_pred_family = classifier.predict(tfidf_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model 1 with BiGram TF IDF Embedding\n",
      "accuracy :  0.8621212121212121\n",
      "f1_score :  0.8463468390633471\n",
      "precision score :  0.87595112439363\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"SVM Model 1 with BiGram TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_family_test, y_pred_family))\n",
    "print('f1_score : ', metrics.f1_score(y_family_test, y_pred_family, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_family_test, y_pred_family, average= 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testons le 1er modèle par cross validation\n",
    "\n",
    "tfidf_1 = TfidfVectorizer(ngram_range=(2,2), max_df=0.8)\n",
    "X = df['cleaned_text'].values.tolist()\n",
    "y_family = df['family'].values.tolist()\n",
    "tfidf_1_X = tfidf_1.fit_transform(X)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 29)\n",
    "K_folds = KFold(n_splits=10)\n",
    "scores = cross_val_score(classifier, tfidf_X, Y, cv=K_folds)\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deuxieme modele\n",
    "#SVM TF IDF Bi Gram\n",
    "# Nous aurons 4 modèles (car 4 familles)\n",
    "\n",
    "X_hl_train = []\n",
    "X_fgsi_train = []\n",
    "X_at_train = []\n",
    "X_cjk_train = []\n",
    "\n",
    "y_hl_train = [] \n",
    "y_fgsi_train = []\n",
    "y_at_train = []\n",
    "y_cjk_train = []\n",
    "\n",
    "def create_train_sets_model2(X, y):\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 'HIN/TEL':\n",
    "            X_hl_train.append(X[i])\n",
    "            txt = X[i]\n",
    "            idx = df.query('cleaned_text ==@txt')['origin'].index.values\n",
    "            y_hl_train.append(df['origin'][idx[0]])\n",
    "        elif y[i] == 'CHI/JPN/KOR':\n",
    "            X_cjk_train.append(X[i])\n",
    "            txt = X[i]\n",
    "            idx = df.query('cleaned_text ==@txt')['origin'].index.values\n",
    "            y_cjk_train.append(df['origin'][idx[0]])\n",
    "        elif y[i] == 'FRE/GER/SPA/ITA':\n",
    "            X_fgsi_train.append(X[i])\n",
    "            txt = X[i]\n",
    "            idx = df.query('cleaned_text ==@txt')['origin'].index.values\n",
    "            y_fgsi_train.append(df['origin'][idx[0]])\n",
    "        else:\n",
    "            X_at_train.append(X[i])\n",
    "            txt = X[i]\n",
    "            idx = df.query('cleaned_text ==@txt')['origin'].index.values\n",
    "            y_at_train.append(df['origin'][idx[0]])\n",
    "    \n",
    "    i = 0        \n",
    "    while i in range(int(len(y_hl_train )/ 2)):\n",
    "        rdx = random.randrange(len(y))\n",
    "        if y[rdx] != 'HIN/TEL':\n",
    "            X_hl_train.append(df['cleaned_text'][rdx])\n",
    "            y_hl_train.append('UNK')\n",
    "            i = i + 1\n",
    "            \n",
    "    i = 0 \n",
    "    while i in range(int(len(y_cjk_train )/ 3)):\n",
    "        rdx = random.randrange(len(y))\n",
    "        if y[rdx] != 'CHI/JPN/KOR':\n",
    "            X_cjk_train.append(df['cleaned_text'][rdx])\n",
    "            y_cjk_train.append('UNK')\n",
    "            i = i + 1\n",
    "            \n",
    "    i = 0 \n",
    "    while i in range(int(len(y_at_train) / 2)):\n",
    "        rdx = random.randrange(len(y))\n",
    "        if y[rdx] != 'ARA/TUR':\n",
    "            X_at_train.append(df['cleaned_text'][rdx])\n",
    "            y_at_train.append('UNK')\n",
    "            i = i + 1\n",
    "            \n",
    "    i = 0 \n",
    "    while i in range(int(len(y_fgsi_train )/ 4)):\n",
    "        rdx = random.randrange(len(y))\n",
    "        if y[rdx] != 'FRE/GER/SPA/ITA':\n",
    "            X_fgsi_train.append(df['cleaned_text'][rdx])\n",
    "            y_fgsi_train.append('UNK')\n",
    "            i = i + 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hl_test = []\n",
    "X_fgsi_test = []\n",
    "X_at_test = []\n",
    "X_cjk_test = []\n",
    "y_hl_test = [] \n",
    "y_fgsi_test = []\n",
    "y_at_test = []\n",
    "y_cjk_test = []\n",
    "\n",
    "\n",
    "def create_test_sets_model2(X, y):\n",
    "    \n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 'HIN/TEL':\n",
    "            X_hl_test.append(X[i])\n",
    "            txt = X[i]\n",
    "            idx = df.query('cleaned_text ==@txt')['origin'].index.values\n",
    "            y_hl_test.append(df['origin'][idx[0]])\n",
    "        elif y[i] == 'CHI/JPN/KOR':\n",
    "            X_cjk_test.append(X[i])\n",
    "            txt = X[i]\n",
    "            idx = df.query('cleaned_text ==@txt')['origin'].index.values\n",
    "            y_cjk_test.append(df['origin'][idx[0]])\n",
    "        elif y[i] == 'FRE/GER/SPA/ITA':\n",
    "            X_fgsi_test.append(X[i])\n",
    "            txt = X[i]\n",
    "            idx = df.query('cleaned_text ==@txt')['origin'].index.values\n",
    "            y_fgsi_test.append(df['origin'][idx[0]])\n",
    "        else:\n",
    "            X_at_test.append(X[i])\n",
    "            txt = X[i]\n",
    "            idx = df.query('cleaned_text ==@txt')['origin'].index.values\n",
    "            y_at_test.append(df['origin'][idx[0]])\n",
    "            \n",
    "    i = 0        \n",
    "    while i in range(int(len(y_hl_test)/ 2)):\n",
    "        rdx = random.randrange(len(y))\n",
    "        if y[rdx] != 'HIN/TEL':\n",
    "            X_hl_test.append(df['cleaned_text'][rdx])\n",
    "            y_hl_test.append('UNK')\n",
    "            i = i + 1\n",
    "            \n",
    "    i = 0 \n",
    "    while i in range(int(len(y_cjk_test )/ 3)):\n",
    "        rdx = random.randrange(len(y))\n",
    "        if y[rdx] != 'CHI/JPN/KOR':\n",
    "            X_cjk_test.append(df['cleaned_text'][rdx])\n",
    "            y_cjk_test.append('UNK')\n",
    "            i = i + 1\n",
    "            \n",
    "    i = 0 \n",
    "    while i in range(int(len(y_at_test )/ 2)):\n",
    "        rdx = random.randrange(len(y))\n",
    "        if y[rdx] != 'ARA/TUR':\n",
    "            X_at_test.append(df['cleaned_text'][rdx])\n",
    "            y_at_test.append('UNK')\n",
    "            i = i + 1\n",
    "            \n",
    "    i = 0 \n",
    "    while i in range(int(len(y_fgsi_test )/ 4)):\n",
    "        rdx = random.randrange(len(y))\n",
    "        if y[rdx] != 'FRE/GER/SPA/ITA':\n",
    "            X_fgsi_test.append(df['cleaned_text'][rdx])\n",
    "            y_fgsi_test.append('UNK')\n",
    "            i = i + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des datasets des 4 sous modèles \n",
    "create_train_sets_model2(X_train, y_family_train)\n",
    "create_test_sets_model2(X_test, y_pred_family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On entraine et on test les 4 sous modèles\n",
    "\n",
    "# Model Hindi Telugu\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,2), max_df= 0.8)\n",
    "X = X_hl_train + X_hl_test\n",
    "tfidf_X = tfidf.fit_transform(X)\n",
    "tfidf_train = tfidf.transform(X_hl_train)\n",
    "tfidf_test = tfidf.transform(X_hl_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = SVC(kernel = 'linear', random_state = 29)\n",
    "classifier.fit(tfidf_train, y_hl_train)\n",
    "#Prediction sur le Test set\n",
    "y_pred_hl = classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model 2 Telugu Hindi with BiGram TF IDF Embedding\n",
      "accuracy :  0.6909361069836553\n",
      "f1_score :  0.19942206131464443\n",
      "precision score :  0.19996170158441964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"SVM Model 2 Telugu Hindi with BiGram TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_hl_test, y_pred_hl))\n",
    "print('f1_score : ', metrics.f1_score(y_hl_test, y_pred_hl, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_hl_test, y_pred_hl, average= 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Arabe turque\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,2), max_df= 0.8)\n",
    "X = X_at_train + X_at_test\n",
    "tfidf_X = tfidf.fit_transform(X)\n",
    "tfidf_train = tfidf.transform(X_at_train)\n",
    "tfidf_test = tfidf.transform(X_at_test)\n",
    "classifier = SVC(kernel = 'linear', random_state = 29)\n",
    "classifier.fit(tfidf_train, y_at_train)\n",
    "#Prediction sur le Test set\n",
    "y_pred_at = classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model 2 Arabic Turkish with BiGram TF IDF Embedding\n",
      "accuracy :  0.6472945891783567\n",
      "f1_score :  0.15818434585617477\n",
      "precision score :  0.17282925782925784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"SVM Model 2 Arabic Turkish with BiGram TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_at_test, y_pred_at))\n",
    "print('f1_score : ', metrics.f1_score(y_at_test, y_pred_at, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_at_test, y_pred_at, average= 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Chinois Japonais Coréen\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,2), max_df= 0.8)\n",
    "X = X_cjk_train + X_cjk_test\n",
    "tfidf_X = tfidf.fit_transform(X)\n",
    "tfidf_train = tfidf.transform(X_cjk_train)\n",
    "tfidf_test = tfidf.transform(X_cjk_test)\n",
    "classifier = SVC(kernel = 'linear', random_state = 29)\n",
    "classifier.fit(tfidf_train, y_cjk_train)\n",
    "#Prediction sur le Test set\n",
    "y_pred_cjk = classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model 2 Chinese Japanese and Korean with BiGram TF IDF Embedding\n",
      "accuracy :  0.6004784688995215\n",
      "f1_score :  0.2111401775629068\n",
      "precision score :  0.20359594118026916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"SVM Model 2 Chinese Japanese and Korean with BiGram TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_cjk_test, y_pred_cjk))\n",
    "print('f1_score : ', metrics.f1_score(y_cjk_test, y_pred_cjk, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_cjk_test, y_pred_cjk, average= 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Français allemand espagnol italien\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,2), max_df= 0.8)\n",
    "X = X_fgsi_train + X_fgsi_test\n",
    "tfidf_X = tfidf.fit_transform(X)\n",
    "tfidf_train = tfidf.transform(X_fgsi_train)\n",
    "tfidf_test = tfidf.transform(X_fgsi_test)\n",
    "classifier = SVC(kernel = 'linear', random_state = 29)\n",
    "classifier.fit(tfidf_train, y_fgsi_train)\n",
    "#Prediction sur le Test set\n",
    "y_pred_fgsi = classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model 2 French German Spanish and Italian with BiGram TF IDF Embedding\n",
      "accuracy :  0.5543575920934412\n",
      "f1_score :  0.2550747985897411\n",
      "precision score :  0.8287652958373828\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"SVM Model 2 French German Spanish and Italian with BiGram TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_fgsi_test, y_pred_fgsi))\n",
    "print('f1_score : ', metrics.f1_score(y_fgsi_test, y_pred_fgsi, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_fgsi_test, y_pred_fgsi, average= 'macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tester le modèle le plus performant sur les données tests\n",
    "\n",
    "Vous pouvez commencer ici ou Restart le kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM TFIDF BIGRAM CROSS VALIDATION K_fold = 10 \n",
    "# 46 min de compilation\n",
    "# Si vous ne souhaitez pas attendre autant de temps, vous pouvez utiliser le modèle sans cross validation en bas (dernière cellule)\n",
    "# 0.72 pour le validation set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import contractions\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt') as file:\n",
    "    lines = file.readlines()\n",
    "    with open('test.txt', \"w\") as file:\n",
    "        for line in lines:\n",
    "            file.write(re.sub(r\"\\(([A-Z]+)\\) (.+)\", r'\\1;\"\\2\"', line))\n",
    "# Upload du fichier\n",
    "df = pd.read_table(\"test.txt\", sep=\";\", names=[\"origin\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #lowercase\n",
    "    res_text = text.lower()\n",
    "    #contactions\n",
    "    res_text = contractions.fix(res_text)\n",
    "    #remove punctuation\n",
    "    res_text = re.sub(r'[^\\w\\s]', '', res_text)\n",
    "\n",
    "    return res_text\n",
    "cleaned_text = [clean_text(df.iloc[i]['text']) for i in range(len(df))]\n",
    "df['cleaned_text'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule à executer pour SVM avec cross validation\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(2,2), max_df=0.8)\n",
    "X = df['cleaned_text'].values.tolist()\n",
    "Y = df['origin'].values.tolist()\n",
    "tfidf_X = tfidf.fit_transform(X)\n",
    "# Utilisation du modèle SVM\n",
    "\n",
    "classifier = SVC(kernel = 'linear', random_state = 29)\n",
    "K_folds = KFold(n_splits=10)\n",
    "scores = cross_val_score(classifier, tfidf_X, Y, cv=K_folds)\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule à executer pour SVM sans cross validation\n",
    "X = df['cleaned_text'].values.tolist()\n",
    "Y = df['origin'].values.tolist()\n",
    "tf = TfidfVectorizer(max_df=0.8, ngram_range=(2,2))\n",
    "tf_X = tf.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=29)\n",
    "tf_train = tf.transform(X_train)\n",
    "tf_test = tf.transform(X_test)\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(tf_train, y_train)\n",
    "tf_y_pred = classifier.predict(tf_test)\n",
    "from sklearn import metrics\n",
    "print(\"SVM with TriGram TF IDF Embedding\")\n",
    "print('accuracy : ', metrics.accuracy_score(y_test, tf_y_pred))\n",
    "print('f1_score : ', metrics.f1_score(y_test, tf_y_pred, average='macro'))\n",
    "print('precision score : ', metrics.precision_score(y_test, tf_y_pred, average= 'macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
